import tokenizer

token_reader = tokenizer.deterministic_automata(
    [
        [11,25,2,19,100,100,100,100,15,100,100,100,100,100,1,102,26,100,100], #0
        [24,2,2,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101,101], #1
        [2,2,2,101,101,101,101,5,101,101,101,101,101,101,8,3,101,200,101], #2
        [4,4,4,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102,102], #3
        [4,4,4,102,102,102,102,5,102,102,102,102,102,102,102,102,102,201,102], #4
        [6,6,6,103,103,103,103,103,103,103,103,103,103,103,7,103,103,103,103], #5
        [6,6,6,103,103,103,103,103,103,103,103,103,103,103,103,103,103,202,103], #6
        [6,6,6,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103,103], #7
        [9,9,9,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104], #8
        [9,9,9,104,104,104,104,104,104,104,104,104,10,104,104,104,104,104,104], #9
        [104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,104,203,104], #10
        [12,12,101,101,101,101,101,101,101,101,101,13,101,101,101,3,101,200,101], #11
        [12,12,0,105,105,105,105,105,105,105,105,105,105,105,105,105,105,204,105], #12
        [14,14,14,14,106,106,106,106,106,106,106,106,106,14,106,106,106,106,106], #13
        [14,14,14,14,106,106,106,106,106,106,16,106,106,14,106,106,106,205,106], #14
        [107,107,107,107,107,107,107,107,107,16,107,107,107,107,107,107,107,107,107], #15
        [107,107,107,107,107,107,107,107,107,107,17,107,107,107,107,107,107,107,107], #16
        [107,107,107,107,107,107,107,18,107,107,107,107,107,107,107,107,107,107,107], #17
        [107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,206,107], #18
        [107,107,107,107,20,107,107,107,107,107,107,107,107,107,107,107,107,107,107], #19
        [107,107,107,107,107,21,107,107,107,107,107,107,107,107,107,107,107,107,107], #20
        [107,107,107,107,107,107,22,107,107,107,107,107,107,107,107,107,107,107,107], #21
        [107,107,107,107,107,107,107,23,107,107,107,107,107,107,107,107,107,107,107], #22
        [107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,107,206,107], #23
        [101,101,101,101,101,101,101,101,101,101,101,106,101,101,101,3,101,101,101], #24
        [25,25,2,105,105,105,105,5,105,105,105,105,105,105,8,3,105,204,105], #25
        [26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,26,207,26,26]
    ],
    {
        '0' : 0,
        '1' : 1,
        '2' : 2,
        '3' : 2,
        '4' : 2,
        '5' : 2,
        '6' : 2,
        '7' : 2,
        '8' : 2,
        '9' : 2,
        'F' : 3,
        'a' : 4,
        'l' : 5,
        's' : 6,
        'e' : 7,
        'T' : 8,
        'r' : 9,
        'u' : 10,
        'x' : 11,
        'j' : 12,
        'A' : 13,
        'B' : 13,
        'C' : 13,
        'D' : 13,
        'E' : 13,
        '+' : 14,
        '-' : 14,
        '.' : 15,
        '"' : 16,
        #Delimiters
        ' ' : 17,
        '\n' : 17,
    },
    {
        100 : 'Non-recognized',
        101 : 'Malformed integer',
        102 : 'Malformed float',
        103 : 'Malformed scientific notation',
        104 : 'Malformed complex',
        105 : 'Malformed binary',
        106 : 'Malformed hexadecimal',
        107 : 'Malformed boolean',
        108 : 'Malformed string',
        200 : 'Integer',
        201 : 'Float',
        202 : 'Scientific notation',
        203 : 'Complex',
        204 : 'Binary',
        205 : 'Hexadecimal',
        206 : 'Boolean',
        207 : 'String',
    }
)

file = open("input.txt","r")
data = file.readlines()
state = 0
num = ''

for line in data:
    tokens = token_reader.tokenize(line, " ")

    print(f'{line[:-1]}, {tokens}')
        